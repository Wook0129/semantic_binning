{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from semantic_binning import SemanticBinning\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, data_path, var_dict, n_bins_range=range(2, 5),\n",
    "                 n_init_bins_list=[5, 10, 15, 20], random_state=42):\n",
    "        \n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.var_dict = var_dict\n",
    "        self.n_bins_range = n_bins_range\n",
    "        self.n_init_bins_list = n_init_bins_list\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.semantic_binning = SemanticBinning(self.var_dict, embedding_dim=4,\n",
    "                                                max_iter=100, verbose=False)\n",
    "        self.class_var = self.data[var_dict['class_var']]\n",
    "        self.n_class = len(self.class_var.unique())\n",
    "    \n",
    "    def _make_cv_folds(self, cv):\n",
    "\n",
    "        cv_folds = []\n",
    "        idxs = np.arange(0, len(self.data))\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        folds = np.array_split(idxs, cv)\n",
    "\n",
    "        for i in range(cv):\n",
    "            train_idxs = np.concatenate([x for x in folds[:i] + folds[i+1:]])\n",
    "            val_idxs = folds[i]\n",
    "            cv_folds.append((train_idxs, val_idxs))\n",
    "\n",
    "        return cv_folds\n",
    "\n",
    "    def _get_classification_score(self, trn_x, val_x, trn_y, val_y, models):\n",
    "            \n",
    "        score = dict()\n",
    "\n",
    "        if 'DT' in models:\n",
    "            dt = DecisionTreeClassifier(min_samples_leaf=0.01, random_state=self.random_state)\n",
    "            dt.fit(trn_x, trn_y)\n",
    "            score['DT'] = accuracy_score(val_y, dt.predict(val_x))\n",
    "\n",
    "        if 'LR' in models:\n",
    "            lr = LogisticRegression(C=1.0, random_state=self.random_state)\n",
    "            lr.fit(trn_x, trn_y)\n",
    "            score['LR'] = accuracy_score(val_y, lr.predict(val_x))\n",
    "\n",
    "        if 'NB' in models:\n",
    "            nb = BernoulliNB(binarize=None)\n",
    "            nb.fit(trn_x, trn_y)\n",
    "            score['NB'] = accuracy_score(val_y, nb.predict(val_x))\n",
    "\n",
    "        return score\n",
    "        \n",
    "    def classification_performance(self, cv=3):\n",
    "        \n",
    "        raw_scores, sc_scores = list(), list()\n",
    "        ewb_scores, efb_scores, sb_scores = dict(), dict(), dict()\n",
    "        \n",
    "        for trn_idx, val_idx in self._make_cv_folds(cv):\n",
    "            \n",
    "            trn_x, val_x = self.data.loc[trn_idx], self.data.loc[val_idx]\n",
    "            trn_y, val_y = self.class_var[trn_idx], self.class_var[val_idx]\n",
    "            \n",
    "            trn_data_handler = DataHandler(trn_x, self.var_dict)\n",
    "            val_data_handler = DataHandler(val_x, self.var_dict)\n",
    "            \n",
    "            # Dummy Coding Only\n",
    "            trn_raw = trn_data_handler.get_dummy_coded_data('dummy_only')\n",
    "            val_raw = val_data_handler.get_dummy_coded_data('dummy_only')\n",
    "            raw_score = self._get_classification_score(trn_raw, val_raw, trn_y, val_y, ['DT', 'LR'])\n",
    "            raw_scores.append(raw_score)\n",
    "            \n",
    "            # Dummy Coding + Scaling for Numerical Vars\n",
    "            scaler = StandardScaler()\n",
    "            numerical_vars = self.var_dict['numerical_vars']\n",
    "            trn_sc, val_sc = trn_x.copy(), val_x.copy()\n",
    "            trn_sc[numerical_vars] = scaler.fit_transform(trn_sc[numerical_vars])\n",
    "            val_sc[numerical_vars] = scaler.transform(val_sc[numerical_vars])\n",
    "            trn_sc = DataHandler(trn_sc, self.var_dict).get_dummy_coded_data('dummy_only')\n",
    "            val_sc = DataHandler(val_sc, self.var_dict).get_dummy_coded_data('dummy_only')\n",
    "            \n",
    "            sc_score = self._get_classification_score(trn_sc, val_sc, trn_y, val_y, ['DT', 'LR'])            \n",
    "            sc_scores.append(sc_score)\n",
    "\n",
    "            for n_bins in self.n_bins_range:\n",
    "                \n",
    "                # Equal Width Binning\n",
    "                trn_ewb = trn_data_handler.get_dummy_coded_data('equal_width', n_bins)\n",
    "                ewb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_ewb)\n",
    "                val_ewb = val_data_handler.get_dummy_coded_data(bins_by_variable=ewb_bins)\n",
    "                ewb_score = self._get_classification_score(trn_ewb, val_ewb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_bins not in ewb_scores:\n",
    "                    ewb_scores[n_bins] = [ewb_score]\n",
    "                else:\n",
    "                    ewb_scores[n_bins].append(ewb_score)\n",
    "                \n",
    "                # Equal Freq Binning\n",
    "                trn_efb = trn_data_handler.get_dummy_coded_data('equal_freq', n_bins)\n",
    "                efb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_efb)\n",
    "                val_efb = val_data_handler.get_dummy_coded_data(bins_by_variable=efb_bins)\n",
    "                efb_score = self._get_classification_score(trn_efb, val_efb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_bins not in efb_scores:\n",
    "                    efb_scores[n_bins] = [efb_score]\n",
    "                else:\n",
    "                    efb_scores[n_bins].append(efb_score)\n",
    "                    \n",
    "            for n_init_bins in self.n_init_bins_list:\n",
    "\n",
    "                # Semantic Binning\n",
    "                trn_sb = self.semantic_binning.fit_transform(trn_x, n_init_bins)\n",
    "                val_sb = self.semantic_binning.transform(val_x)\n",
    "                sb_score = self._get_classification_score(trn_sb, val_sb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_init_bins not in sb_scores:\n",
    "                    sb_scores[n_init_bins] = [sb_score]\n",
    "                else:\n",
    "                    sb_scores[n_init_bins].append(sb_score)\n",
    "                    \n",
    "        scores = dict(raw_scores=raw_scores, sc_scores=sc_scores,\n",
    "                      ewb_scores=ewb_scores, efb_scores=efb_scores,\n",
    "                      sb_scores=sb_scores)\n",
    "        return scores\n",
    "    \n",
    "    def clustering_performance(self, methods=['kmeans', 'agglomerative']):\n",
    "        \n",
    "        def get_clustering_score(X, method):\n",
    "            if method == 'kmeans':\n",
    "                cluster_label = KMeans(n_clusters=self.n_class, \n",
    "                                       random_state=self.random_state).fit_predict(X)\n",
    "            if method == 'agglomerative':\n",
    "                cluster_label = AgglomerativeClustering(n_clusters=self.n_class).fit_predict(X)\n",
    "            return adjusted_mutual_info_score(self.class_var, cluster_label)\n",
    "        \n",
    "        scores = dict()\n",
    "        \n",
    "        data_handler = DataHandler(self.data, self.var_dict)\n",
    "        \n",
    "        scores['dummy_only'] = dict()\n",
    "        dummy_coded = data_handler.get_dummy_coded_data('dummy_only')\n",
    "        for method in methods:\n",
    "            dummy_score = get_clustering_score(dummy_coded, method)\n",
    "            scores['dummy_only'][method] = dummy_score\n",
    "        \n",
    "        scores['scale_numeric'] = dict()\n",
    "        scale_numeric = data_handler.get_dummy_coded_data('scale_numeric')\n",
    "        for method in methods:\n",
    "            scale_score = get_clustering_score(scale_numeric, method)\n",
    "            scores['scale_numeric'][method] = scale_score\n",
    "\n",
    "        scores['equal_width'] = dict()\n",
    "        for method in methods:\n",
    "            scores['equal_width'][method] = dict()\n",
    "            for n_bins in self.n_bins_range:\n",
    "                ewb = data_handler.get_dummy_coded_data('equal_width', n_bins)\n",
    "                ewb_score = get_clustering_score(ewb, method)\n",
    "                scores['equal_width'][method][n_bins] = ewb_score\n",
    "\n",
    "        scores['equal_freq'] = dict()\n",
    "        for method in methods:\n",
    "            scores['equal_freq'][method] = dict()\n",
    "            for n_bins in self.n_bins_range:   \n",
    "                efb = data_handler.get_dummy_coded_data('equal_freq', n_bins)\n",
    "                efb_score = get_clustering_score(efb, method)\n",
    "                scores['equal_freq'][method][n_bins] = efb_score\n",
    "        \n",
    "        scores['semantic_binning'] = dict()\n",
    "        for method in methods:\n",
    "            scores['semantic_binning'][method] = dict()\n",
    "        for n_init_bins in self.n_init_bins_list:\n",
    "            sb = self.semantic_binning.fit_transform(self.data, n_init_bins)\n",
    "            for method in methods:\n",
    "                sb_score = get_clustering_score(sb, method)\n",
    "                scores['semantic_binning'][method][n_init_bins] = sb_score\n",
    "                \n",
    "        return scores\n",
    "\n",
    "def compute_cv_score(scores, models=['DT', 'LR']):\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        cv_score = np.array([fold[model] for fold in scores])\n",
    "        print('Accuracy = {0:0.3f} (+/- {1:0.3f})'.format(cv_score.mean(), cv_score.std() * 2))\n",
    "        print('')\n",
    "\n",
    "def compute_cv_score_by_n_bins(scores, models=['DT', 'LR', 'NB']):\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        for n_bins in scores:\n",
    "            cv_score = np.array([fold[model] for fold in scores[n_bins]])\n",
    "            print('#Bins = {0}, Accuracy = {1:0.3f} (+/- {2:0.3f})'.format(n_bins, cv_score.mean(), cv_score.std() * 2))\n",
    "        print('')\n",
    "        \n",
    "def print_clustering_score(scores):\n",
    "    for method, score in scores.items():\n",
    "        print('{0}, NMI = {1:0.4f}'.format(method, score))\n",
    "    \n",
    "def print_clustering_score_by_n_bins(scores):\n",
    "    for method, score_by_n_bins in scores.items():\n",
    "        print(method)\n",
    "        for n_bins, score in score_by_n_bins.items():\n",
    "            print('#Bins = {0}, NMI = {1:0.4f}'.format(n_bins, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
       "       'promotion_last_5years', 'sales', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_data = pd.read_csv('data/HR_comma_sep.csv')\n",
    "hr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_dict = dict(\n",
    "    categorical_vars = ['Work_accident', 'promotion_last_5years', 'sales', \n",
    "                        'salary', 'number_project','time_spend_company'],\n",
    "    numerical_vars = ['satisfaction_level', 'last_evaluation', 'average_montly_hours'],\n",
    "    class_var = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = Experiment('data/HR_comma_sep.csv', \n",
    "                 var_dict, \n",
    "                 n_bins_range=range(2, 21),\n",
    "                 n_init_bins_list=[5, 10, 15, 20, 25, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_scores = exp.classification_performance(cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Accuracy = 0.959 (+/- 0.003)\n",
      "\n",
      "LR\n",
      "Accuracy = 0.896 (+/- 0.008)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score(clf_scores['raw_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Accuracy = 0.959 (+/- 0.003)\n",
      "\n",
      "LR\n",
      "Accuracy = 0.896 (+/- 0.008)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score(clf_scores['sc_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 2, Accuracy = 0.941 (+/- 0.005)\n",
      "#Bins = 3, Accuracy = 0.949 (+/- 0.006)\n",
      "#Bins = 4, Accuracy = 0.941 (+/- 0.002)\n",
      "\n",
      "LR\n",
      "#Bins = 2, Accuracy = 0.894 (+/- 0.010)\n",
      "#Bins = 3, Accuracy = 0.912 (+/- 0.008)\n",
      "#Bins = 4, Accuracy = 0.906 (+/- 0.006)\n",
      "\n",
      "NB\n",
      "#Bins = 2, Accuracy = 0.843 (+/- 0.011)\n",
      "#Bins = 3, Accuracy = 0.889 (+/- 0.010)\n",
      "#Bins = 4, Accuracy = 0.870 (+/- 0.011)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(clf_scores['ewb_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 2, Accuracy = 0.929 (+/- 0.013)\n",
      "#Bins = 3, Accuracy = 0.945 (+/- 0.005)\n",
      "#Bins = 4, Accuracy = 0.926 (+/- 0.008)\n",
      "\n",
      "LR\n",
      "#Bins = 2, Accuracy = 0.892 (+/- 0.008)\n",
      "#Bins = 3, Accuracy = 0.914 (+/- 0.008)\n",
      "#Bins = 4, Accuracy = 0.908 (+/- 0.005)\n",
      "\n",
      "NB\n",
      "#Bins = 2, Accuracy = 0.853 (+/- 0.014)\n",
      "#Bins = 3, Accuracy = 0.882 (+/- 0.013)\n",
      "#Bins = 4, Accuracy = 0.870 (+/- 0.012)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(clf_scores['efb_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 5, Accuracy = 0.934 (+/- 0.015)\n",
      "#Bins = 10, Accuracy = 0.936 (+/- 0.008)\n",
      "#Bins = 15, Accuracy = 0.930 (+/- 0.011)\n",
      "#Bins = 20, Accuracy = 0.942 (+/- 0.013)\n",
      "\n",
      "LR\n",
      "#Bins = 5, Accuracy = 0.901 (+/- 0.021)\n",
      "#Bins = 10, Accuracy = 0.924 (+/- 0.003)\n",
      "#Bins = 15, Accuracy = 0.924 (+/- 0.024)\n",
      "#Bins = 20, Accuracy = 0.958 (+/- 0.001)\n",
      "\n",
      "NB\n",
      "#Bins = 5, Accuracy = 0.864 (+/- 0.023)\n",
      "#Bins = 10, Accuracy = 0.880 (+/- 0.013)\n",
      "#Bins = 15, Accuracy = 0.885 (+/- 0.026)\n",
      "#Bins = 20, Accuracy = 0.927 (+/- 0.011)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(clf_scores['sb_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 44.7 s, total: 2min 58s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%time clustering_scores = exp.clustering_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dummy_only', 'scale_numeric', 'equal_width', 'equal_freq', 'semantic_binning'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans, NMI = 0.0017\n",
      "agglomerative, NMI = 0.0103\n"
     ]
    }
   ],
   "source": [
    "print_clustering_score(clustering_scores['dummy_only'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans, NMI = 0.0004\n",
      "agglomerative, NMI = 0.2091\n"
     ]
    }
   ],
   "source": [
    "print_clustering_score(clustering_scores['scale_numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans\n",
      "#Bins = 2, NMI = 0.0001\n",
      "#Bins = 3, NMI = 0.0279\n",
      "#Bins = 4, NMI = 0.0131\n",
      "agglomerative\n",
      "#Bins = 2, NMI = 0.1436\n",
      "#Bins = 3, NMI = 0.2005\n",
      "#Bins = 4, NMI = 0.2232\n"
     ]
    }
   ],
   "source": [
    "print_clustering_score_by_n_bins(clustering_scores['equal_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans\n",
      "#Bins = 2, NMI = 0.0011\n",
      "#Bins = 3, NMI = 0.0934\n",
      "#Bins = 4, NMI = 0.0131\n",
      "agglomerative\n",
      "#Bins = 2, NMI = 0.0144\n",
      "#Bins = 3, NMI = 0.2531\n",
      "#Bins = 4, NMI = 0.1808\n"
     ]
    }
   ],
   "source": [
    "print_clustering_score_by_n_bins(clustering_scores['equal_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans\n",
      "#Bins = 5, NMI = 0.0131\n",
      "#Bins = 10, NMI = 0.0131\n",
      "#Bins = 15, NMI = 0.0131\n",
      "#Bins = 20, NMI = 0.0131\n",
      "agglomerative\n",
      "#Bins = 5, NMI = 0.0290\n",
      "#Bins = 10, NMI = 0.1874\n",
      "#Bins = 15, NMI = 0.1972\n",
      "#Bins = 20, NMI = 0.0781\n"
     ]
    }
   ],
   "source": [
    "print_clustering_score_by_n_bins(clustering_scores['semantic_binning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
