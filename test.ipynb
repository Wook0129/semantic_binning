{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_handler import DataHandler\n",
    "from semantic_binning import SemanticBinning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
       "       'promotion_last_5years', 'sales', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_data = pd.read_csv('data/HR_comma_sep.csv')\n",
    "hr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_dict = dict(\n",
    "    categorical_vars = ['Work_accident', 'promotion_last_5years', 'sales', \n",
    "                        'salary', 'number_project','time_spend_company'],\n",
    "    numerical_vars = ['satisfaction_level', 'last_evaluation', 'average_montly_hours'],\n",
    "    class_var = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from semantic_binning import SemanticBinning\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, data_path, var_dict):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.var_dict = var_dict\n",
    "        self.semantic_binning = SemanticBinning(self.var_dict, embedding_dim=8, \n",
    "                                                max_iter=300000)\n",
    "        self.class_var = self.data[var_dict['class_var']]\n",
    "        self.n_class = len(self.class_var.unique())\n",
    "    \n",
    "    def classification_performance(self):\n",
    "        data_handler = DataHandler(self.data, self.var_dict)\n",
    "        data_handler\n",
    "        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def clustering_performance(self, method='agglomerative'):\n",
    "        \n",
    "        def get_score(X, method):\n",
    "            if method == 'kmeans':\n",
    "                cluster_label = KMeans(n_clusters=self.n_class).fit_predict(X)\n",
    "            if method == 'agglomerative':\n",
    "                cluster_label = AgglomerativeClustering(n_clusters=self.n_class).fit_predict(X)\n",
    "            return adjusted_mutual_info_score(self.class_var, cluster_label)\n",
    "        \n",
    "        data_handler = DataHandler(self.data, self.var_dict)\n",
    "        \n",
    "        dummy_coded = data_handler.get_dummy_coded_data(init_discretize_method='dummy_only')\n",
    "        scale_numeric = data_handler.get_dummy_coded_data(init_discretize_method='scale_numeric')\n",
    "        ewb = data_handler.get_dummy_coded_data(init_discretize_method='equal_width', n_init_bins=10)\n",
    "        efb = data_handler.get_dummy_coded_data(init_discretize_method='equal_freq', n_init_bins=10)\n",
    "        sb = self.semantic_binning.fit_transform(self.data, n_init_bins=10)\n",
    "        \n",
    "        print('Clustering Performance By Discretization Method')\n",
    "        print('> Dummy Coding Only = {}'.format(get_score(dummy_coded, method)))\n",
    "        print('> Scale Numerical Vars = {}'.format(get_score(scale_numeric, method)))\n",
    "        print('> Equal Width Binning = {}'.format(get_score(ewb, method)))\n",
    "        print('> Equal Freq Binning = {}'.format(get_score(efb, method)))\n",
    "        print('> Semantic Binning = {}'.format(get_score(sb, method)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = Experiment('data/HR_comma_sep.csv', var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Iteration = 10000, Loss = 0.2976953387260437\n",
      ">>> Iteration = 20000, Loss = 0.2967853546142578\n",
      ">>> Iteration = 30000, Loss = 0.2917010486125946\n",
      ">>> Iteration = 40000, Loss = 0.2964842617511749\n",
      ">>> Iteration = 50000, Loss = 0.29677262902259827\n",
      ">>> Iteration = 60000, Loss = 0.2947201728820801\n",
      ">>> Iteration = 70000, Loss = 0.2959824502468109\n",
      ">>> Iteration = 80000, Loss = 0.29302340745925903\n",
      ">>> Iteration = 90000, Loss = 0.29622188210487366\n",
      ">>> Iteration = 100000, Loss = 0.29901406168937683\n",
      ">>> Iteration = 110000, Loss = 0.2971545159816742\n",
      ">>> Iteration = 120000, Loss = 0.29810118675231934\n",
      ">>> Iteration = 130000, Loss = 0.29306966066360474\n",
      ">>> Iteration = 140000, Loss = 0.2939961850643158\n",
      ">>> Iteration = 150000, Loss = 0.2978173792362213\n",
      ">>> Iteration = 160000, Loss = 0.29650187492370605\n",
      ">>> Iteration = 170000, Loss = 0.29137149453163147\n",
      ">>> Iteration = 180000, Loss = 0.2971870005130768\n",
      ">>> Iteration = 190000, Loss = 0.293679803609848\n",
      ">>> Iteration = 200000, Loss = 0.2939714193344116\n",
      ">>> Iteration = 210000, Loss = 0.29185304045677185\n",
      ">>> Iteration = 220000, Loss = 0.2946977913379669\n",
      ">>> Iteration = 230000, Loss = 0.29607459902763367\n",
      ">>> Iteration = 240000, Loss = 0.29548513889312744\n",
      ">>> Iteration = 250000, Loss = 0.29306891560554504\n",
      ">>> Iteration = 260000, Loss = 0.2948624789714813\n",
      ">>> Iteration = 270000, Loss = 0.29282838106155396\n",
      ">>> Iteration = 280000, Loss = 0.291799932718277\n",
      ">>> Iteration = 290000, Loss = 0.29546356201171875\n",
      ">>> Iteration = 300000, Loss = 0.29244673252105713\n",
      "Learning Embedding Finished!\n",
      "Clustering Performance By Discretization Method\n",
      "> Dummy Coding Only = 0.010268094012067937\n",
      "> Scale Numerical Vars = 0.2091474260339319\n",
      "> Equal Width Binning = 0.23036512774761073\n",
      "> Equal Freq Binning = 0.22574974749163562\n",
      "> Semantic Binning = 0.2602553393283718\n",
      "CPU times: user 15min 35s, sys: 15 s, total: 15min 50s\n",
      "Wall time: 15min 55s\n"
     ]
    }
   ],
   "source": [
    "%time exp.clustering_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Iteration = 10000, Loss = 0.2975062131881714\n",
      ">>> Iteration = 20000, Loss = 0.29386061429977417\n",
      ">>> Iteration = 30000, Loss = 0.29456761479377747\n",
      ">>> Iteration = 40000, Loss = 0.2939647436141968\n",
      ">>> Iteration = 50000, Loss = 0.29233407974243164\n",
      ">>> Iteration = 60000, Loss = 0.2894141972064972\n",
      ">>> Iteration = 70000, Loss = 0.29430869221687317\n",
      ">>> Iteration = 80000, Loss = 0.29418352246284485\n",
      ">>> Iteration = 90000, Loss = 0.29267388582229614\n",
      ">>> Iteration = 100000, Loss = 0.29420921206474304\n",
      ">>> Iteration = 110000, Loss = 0.2952078580856323\n",
      ">>> Iteration = 120000, Loss = 0.297040730714798\n",
      ">>> Iteration = 130000, Loss = 0.2932296693325043\n",
      ">>> Iteration = 140000, Loss = 0.2955964505672455\n",
      ">>> Iteration = 150000, Loss = 0.2965798079967499\n",
      ">>> Iteration = 160000, Loss = 0.2964361310005188\n",
      ">>> Iteration = 170000, Loss = 0.29272913932800293\n",
      ">>> Iteration = 180000, Loss = 0.2942306399345398\n",
      ">>> Iteration = 190000, Loss = 0.29308053851127625\n",
      ">>> Iteration = 200000, Loss = 0.29546141624450684\n",
      ">>> Iteration = 210000, Loss = 0.2947385609149933\n",
      ">>> Iteration = 220000, Loss = 0.29265496134757996\n",
      ">>> Iteration = 230000, Loss = 0.2947903275489807\n",
      ">>> Iteration = 240000, Loss = 0.2934654653072357\n",
      ">>> Iteration = 250000, Loss = 0.2998376190662384\n",
      ">>> Iteration = 260000, Loss = 0.29712456464767456\n",
      ">>> Iteration = 270000, Loss = 0.29430606961250305\n",
      ">>> Iteration = 280000, Loss = 0.2923724055290222\n",
      ">>> Iteration = 290000, Loss = 0.2946999669075012\n",
      ">>> Iteration = 300000, Loss = 0.2956179082393646\n",
      "Learning Embedding Finished!\n",
      "Clustering Performance By Discretization Method\n",
      "> Dummy Coding Only = 0.001669739101139292\n",
      "> Scale Numerical Vars = 0.0003872797188454803\n",
      "> Equal Width Binning = 0.013115940557326494\n",
      "> Equal Freq Binning = 0.013115940557326494\n",
      "> Semantic Binning = 0.11351939752789995\n",
      "CPU times: user 13min 58s, sys: 27.6 s, total: 14min 25s\n",
      "Wall time: 14min 11s\n"
     ]
    }
   ],
   "source": [
    "%time exp.clustering_performance(method='kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_error_for_model(model, data, var_dict, cv=3):\n",
    "\n",
    "    def split_idxs_for_cross_validation(cv):\n",
    "        idx_splitted = []\n",
    "        idxs = np.arange(0, len(data))\n",
    "        np.random.shuffle(idxs)\n",
    "        folds = np.array_split(idxs, cv)\n",
    "        for i in range(cv):\n",
    "            train_idxs = np.concatenate([x for x in folds[:i] + folds[i+1:]])\n",
    "            val_idxs = folds[i]\n",
    "            idx_splitted.append((train_idxs, val_idxs))\n",
    "        return idx_splitted\n",
    "\n",
    "    def get_val_score(trn_x, val_x):\n",
    "        model.fit(trn_x, trn_y)\n",
    "        score = accuracy_score(val_y, model.predict(val_x))\n",
    "        return score\n",
    "\n",
    "    def print_cv_score(scores, setting_name):\n",
    "        scores = np.array(scores)\n",
    "        print('>> {}'.format(setting_name))\n",
    "        print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    semantic_binning = SemanticBinning(var_dict, max_iter=300000, verbose=False)\n",
    "    class_var = data[var_dict['class_var']]\n",
    "    idx_splitted = split_idxs_for_cross_validation(cv)\n",
    "    \n",
    "    dummy_coded_scores = []\n",
    "    ewb_scores = []\n",
    "    efb_scores = []\n",
    "    sb_scores = []\n",
    "    \n",
    "    for trn_idx, val_idx in idx_splitted:\n",
    "        \n",
    "        trn_y = class_var[trn_idx]\n",
    "        val_y = class_var[val_idx]\n",
    "        \n",
    "        trn_data_handler = DataHandler(data.loc[trn_idx], var_dict)\n",
    "        val_data_handler = DataHandler(data.loc[val_idx], var_dict)\n",
    "        \n",
    "        trn_dummy_coded = trn_data_handler.get_dummy_coded_data('dummy_only')\n",
    "        val_dummy_coded = val_data_handler.get_dummy_coded_data('dummy_only')\n",
    "        dummy_coded_scores.append(get_val_score(trn_dummy_coded, val_dummy_coded))\n",
    "        \n",
    "        trn_ewb = trn_data_handler.get_dummy_coded_data('equal_width', n_init_bins=10)\n",
    "        ewb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_ewb)\n",
    "        val_ewb = val_data_handler.get_dummy_coded_data(bins_by_variable=ewb_bins)\n",
    "        ewb_scores.append(get_val_score(trn_ewb, val_ewb))\n",
    "    \n",
    "        trn_efb = trn_data_handler.get_dummy_coded_data('equal_freq', n_init_bins=10)\n",
    "        efb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_efb)\n",
    "        val_efb = val_data_handler.get_dummy_coded_data(bins_by_variable=efb_bins)\n",
    "        efb_scores.append(get_val_score(trn_efb, val_efb))\n",
    "    \n",
    "        trn_sb = semantic_binning.fit_transform(data.loc[trn_idx], n_init_bins=10)\n",
    "        val_sb = semantic_binning.transform(data.loc[val_idx])\n",
    "        sb_scores.append(get_val_score(trn_sb, val_sb))\n",
    "    \n",
    "    print('{} fold cv score'.format(cv))\n",
    "    print_cv_score(dummy_coded_scores, 'dummy_only')\n",
    "    print_cv_score(ewb_scores, 'equal_width')\n",
    "    print_cv_score(efb_scores, 'equal_freq')\n",
    "    print_cv_score(sb_scores, 'semantic_binning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.896 (+/- 0.004)\n",
      ">> equal_width\n",
      "Accuracy: 0.942 (+/- 0.006)\n",
      ">> equal_freq\n",
      "Accuracy: 0.937 (+/- 0.002)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.908 (+/- 0.010)\n",
      "CPU times: user 41min 52s, sys: 50.2 s, total: 42min 42s\n",
      "Wall time: 42min 40s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(LogisticRegression(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.975 (+/- 0.001)\n",
      ">> equal_width\n",
      "Accuracy: 0.967 (+/- 0.004)\n",
      ">> equal_freq\n",
      "Accuracy: 0.964 (+/- 0.005)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.964 (+/- 0.001)\n",
      "CPU times: user 43min 50s, sys: 43.9 s, total: 44min 34s\n",
      "Wall time: 44min 44s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(DecisionTreeClassifier(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.879 (+/- 0.007)\n",
      ">> equal_width\n",
      "Accuracy: 0.920 (+/- 0.006)\n",
      ">> equal_freq\n",
      "Accuracy: 0.907 (+/- 0.005)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.855 (+/- 0.010)\n",
      "CPU times: user 45min 21s, sys: 44.8 s, total: 46min 6s\n",
      "Wall time: 46min 20s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(BernoulliNB(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
