{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_handler import DataHandler\n",
    "from semantic_binning import SemanticBinning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
       "       'promotion_last_5years', 'sales', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_data = pd.read_csv('data/HR_comma_sep.csv')\n",
    "hr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_dict = dict(\n",
    "    categorical_vars = ['Work_accident', 'promotion_last_5years', 'sales', \n",
    "                        'salary', 'number_project','time_spend_company'],\n",
    "    numerical_vars = ['satisfaction_level', 'last_evaluation', 'average_montly_hours'],\n",
    "    class_var = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from semantic_binning import SemanticBinning\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, data_path, var_dict, n_init_bins_list=[5, 10, 15, 20], random_state=42):\n",
    "        \n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.var_dict = var_dict\n",
    "        self.n_init_bins_list = n_init_bins_list\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.semantic_binning = SemanticBinning(self.var_dict, embedding_dim=4,\n",
    "                                                max_iter=100, verbose=False)\n",
    "        self.class_var = self.data[var_dict['class_var']]\n",
    "        self.n_class = len(self.class_var.unique())\n",
    "    \n",
    "    def _make_cv_folds(self, cv):\n",
    "\n",
    "        cv_folds = []\n",
    "        idxs = np.arange(0, len(self.data))\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        folds = np.array_split(idxs, cv)\n",
    "\n",
    "        for i in range(cv):\n",
    "            train_idxs = np.concatenate([x for x in folds[:i] + folds[i+1:]])\n",
    "            val_idxs = folds[i]\n",
    "            cv_folds.append((train_idxs, val_idxs))\n",
    "\n",
    "        return cv_folds\n",
    "\n",
    "    def _get_classification_score(self, trn_x, val_x, trn_y, val_y, models):\n",
    "            \n",
    "        score = dict()\n",
    "\n",
    "        if 'DT' in models:\n",
    "            dt = DecisionTreeClassifier(min_samples_leaf=0.01, random_state=self.random_state)\n",
    "            dt.fit(trn_x, trn_y)\n",
    "            score['DT'] = accuracy_score(val_y, dt.predict(val_x))\n",
    "\n",
    "        if 'LR' in models:\n",
    "            lr = LogisticRegression(C=1.0, random_state=self.random_state)\n",
    "            lr.fit(trn_x, trn_y)\n",
    "            score['LR'] = accuracy_score(val_y, lr.predict(val_x))\n",
    "\n",
    "        if 'NB' in models:\n",
    "            nb = BernoulliNB(binarize=None)\n",
    "            nb.fit(trn_x, trn_y)\n",
    "            score['NB'] = accuracy_score(val_y, nb.predict(val_x))\n",
    "\n",
    "        return score\n",
    "        \n",
    "    def classification_performance(self, cv=3, random_state=42):\n",
    "\n",
    "        raw_scores = []\n",
    "        sc_scores = []\n",
    "        ewb_scores = dict()\n",
    "        efb_scores = dict()\n",
    "        sb_scores = dict()\n",
    "        \n",
    "        for trn_idx, val_idx in self._make_cv_folds(cv):\n",
    "            \n",
    "            trn_x, val_x = self.data.loc[trn_idx], self.data.loc[val_idx]\n",
    "            trn_y, val_y = self.class_var[trn_idx], self.class_var[val_idx]\n",
    "            \n",
    "            trn_data_handler = DataHandler(trn_x, self.var_dict)\n",
    "            val_data_handler = DataHandler(val_x, self.var_dict)\n",
    "            \n",
    "            # Dummy Coding Only\n",
    "            trn_raw = trn_data_handler.get_dummy_coded_data('dummy_only')\n",
    "            val_raw = val_data_handler.get_dummy_coded_data('dummy_only')\n",
    "            raw_score = self._get_classification_score(trn_raw, val_raw, trn_y, val_y, ['DT', 'LR'])\n",
    "            raw_scores.append(raw_score)\n",
    "            \n",
    "            # Dummy Coding + Scaling for Numerical Vars\n",
    "            scaler = StandardScaler()\n",
    "            numerical_vars = self.var_dict['numerical_vars']\n",
    "            trn_sc, val_sc = trn_x.copy(), val_x.copy()\n",
    "            trn_sc[numerical_vars] = scaler.fit_transform(trn_sc[numerical_vars])\n",
    "            val_sc[numerical_vars] = scaler.transform(val_sc[numerical_vars])\n",
    "            trn_sc = DataHandler(trn_sc, self.var_dict).get_dummy_coded_data('dummy_only')\n",
    "            val_sc = DataHandler(val_sc, self.var_dict).get_dummy_coded_data('dummy_only')\n",
    "            \n",
    "            sc_score = self._get_classification_score(trn_sc, val_sc, trn_y, val_y, ['DT', 'LR'])            \n",
    "            sc_scores.append(sc_score)\n",
    "\n",
    "            for n_bins in range(2, 21):\n",
    "                \n",
    "                # Equal Width Binning\n",
    "                trn_ewb = trn_data_handler.get_dummy_coded_data('equal_width', n_init_bins=n_bins)\n",
    "                ewb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_ewb)\n",
    "                val_ewb = val_data_handler.get_dummy_coded_data(bins_by_variable=ewb_bins)\n",
    "                ewb_score = self._get_classification_score(trn_ewb, val_ewb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_bins not in ewb_scores:\n",
    "                    ewb_scores[n_bins] = [ewb_score]\n",
    "                else:\n",
    "                    ewb_scores[n_bins].append(ewb_score)\n",
    "                \n",
    "                # Equal Freq Binning\n",
    "                trn_efb = trn_data_handler.get_dummy_coded_data('equal_freq', n_init_bins=n_bins)\n",
    "                efb_bins = trn_data_handler.get_bins_by_variable_from_data(trn_efb)\n",
    "                val_efb = val_data_handler.get_dummy_coded_data(bins_by_variable=efb_bins)\n",
    "                efb_score = self._get_classification_score(trn_efb, val_efb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_bins not in efb_scores:\n",
    "                    efb_scores[n_bins] = [efb_score]\n",
    "                else:\n",
    "                    efb_scores[n_bins].append(efb_score)\n",
    "                    \n",
    "            for n_init_bins in self.n_init_bins_list:\n",
    "\n",
    "                # Semantic Binning\n",
    "                trn_sb = self.semantic_binning.fit_transform(trn_x, n_init_bins=n_init_bins)\n",
    "                val_sb = self.semantic_binning.transform(val_x)\n",
    "                sb_score = self._get_classification_score(trn_sb, val_sb, trn_y, val_y, ['DT', 'LR', 'NB'])\n",
    "                if n_init_bins not in sb_scores:\n",
    "                    sb_scores[n_init_bins] = [sb_score]\n",
    "                else:\n",
    "                    sb_scores[n_init_bins].append(sb_score)\n",
    "\n",
    "        return raw_scores, sc_scores, ewb_scores, efb_scores, sb_scores\n",
    "\n",
    "    def clustering_performance(self):\n",
    "        \n",
    "        def get_clustering_score(X, method):\n",
    "            if method == 'kmeans':\n",
    "                cluster_label = KMeans(n_clusters=self.n_class).fit_predict(X)\n",
    "            if method == 'agglomerative':\n",
    "                cluster_label = AgglomerativeClustering(n_clusters=self.n_class).fit_predict(X)\n",
    "            return adjusted_mutual_info_score(self.class_var, cluster_label)\n",
    "        \n",
    "        data_handler = DataHandler(self.data, self.var_dict)\n",
    "        \n",
    "        dummy_coded = data_handler.get_dummy_coded_data(init_discretize_method='dummy_only')\n",
    "        scale_numeric = data_handler.get_dummy_coded_data(init_discretize_method='scale_numeric')\n",
    "        ewb = data_handler.get_dummy_coded_data(init_discretize_method='equal_width', n_init_bins=self.n_init_bins)\n",
    "        efb = data_handler.get_dummy_coded_data(init_discretize_method='equal_freq', n_init_bins=self.n_init_bins)\n",
    "        sb = self.semantic_binning.fit_transform(self.data, n_init_bins=self.n_init_bins)\n",
    "        \n",
    "        print('Clustering Performance By Discretization Method')\n",
    "        \n",
    "        for method in ['kmeans', 'agglomerative']:\n",
    "            print('> Clustering Algorithm = {}'.format(method))\n",
    "            print('>> Dummy Coding Only = {}'.format(get_clustering_score(dummy_coded, method)))\n",
    "            print('>> Scale Numerical Vars = {}'.format(get_clustering_score(scale_numeric, method)))\n",
    "            print('>> Equal Width Binning = {}'.format(get_clustering_score(ewb, method)))\n",
    "            print('>> Equal Freq Binning = {}'.format(get_clustering_score(efb, method)))\n",
    "            print('>> Semantic Binning = {}'.format(get_clustering_score(sb, method)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment('data/HR_comma_sep.csv', var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores, sc_scores, ewb_scores, efb_scores, sb_scores = exp.classification_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv_score(scores, models=['DT', 'LR']):\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        cv_score = np.array([fold[model] for fold in scores])\n",
    "        print('Accuracy = {0:0.3f} (+/- {1:0.3f})'.format(cv_score.mean(), cv_score.std() * 2))\n",
    "\n",
    "def compute_cv_score_by_n_bins(scores, models=['DT', 'LR', 'NB']):\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        for n_bins in scores:\n",
    "            cv_score = np.array([fold[model] for fold in scores[n_bins]])\n",
    "            print('#Bins = {0}, Accuracy = {1:0.3f} (+/- {2:0.3f})'.format(n_bins, cv_score.mean(), cv_score.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Accuracy = 0.959 (+/- 0.003)\n",
      "LR\n",
      "Accuracy = 0.896 (+/- 0.008)\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score(raw_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Accuracy = 0.959 (+/- 0.003)\n",
      "LR\n",
      "Accuracy = 0.896 (+/- 0.008)\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score(sc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 2, Accuracy = 0.941 (+/- 0.005)\n",
      "#Bins = 3, Accuracy = 0.949 (+/- 0.006)\n",
      "#Bins = 4, Accuracy = 0.941 (+/- 0.002)\n",
      "#Bins = 5, Accuracy = 0.937 (+/- 0.008)\n",
      "#Bins = 6, Accuracy = 0.944 (+/- 0.002)\n",
      "#Bins = 7, Accuracy = 0.943 (+/- 0.009)\n",
      "#Bins = 8, Accuracy = 0.937 (+/- 0.003)\n",
      "#Bins = 9, Accuracy = 0.938 (+/- 0.003)\n",
      "LR\n",
      "#Bins = 2, Accuracy = 0.894 (+/- 0.010)\n",
      "#Bins = 3, Accuracy = 0.912 (+/- 0.008)\n",
      "#Bins = 4, Accuracy = 0.906 (+/- 0.006)\n",
      "#Bins = 5, Accuracy = 0.922 (+/- 0.006)\n",
      "#Bins = 6, Accuracy = 0.926 (+/- 0.002)\n",
      "#Bins = 7, Accuracy = 0.941 (+/- 0.004)\n",
      "#Bins = 8, Accuracy = 0.938 (+/- 0.001)\n",
      "#Bins = 9, Accuracy = 0.945 (+/- 0.005)\n",
      "NB\n",
      "#Bins = 2, Accuracy = 0.843 (+/- 0.011)\n",
      "#Bins = 3, Accuracy = 0.889 (+/- 0.010)\n",
      "#Bins = 4, Accuracy = 0.870 (+/- 0.011)\n",
      "#Bins = 5, Accuracy = 0.881 (+/- 0.008)\n",
      "#Bins = 6, Accuracy = 0.892 (+/- 0.009)\n",
      "#Bins = 7, Accuracy = 0.905 (+/- 0.002)\n",
      "#Bins = 8, Accuracy = 0.908 (+/- 0.003)\n",
      "#Bins = 9, Accuracy = 0.914 (+/- 0.006)\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(ewb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 2, Accuracy = 0.929 (+/- 0.013)\n",
      "#Bins = 3, Accuracy = 0.945 (+/- 0.005)\n",
      "#Bins = 4, Accuracy = 0.926 (+/- 0.008)\n",
      "#Bins = 5, Accuracy = 0.942 (+/- 0.001)\n",
      "#Bins = 6, Accuracy = 0.943 (+/- 0.004)\n",
      "#Bins = 7, Accuracy = 0.938 (+/- 0.004)\n",
      "#Bins = 8, Accuracy = 0.939 (+/- 0.008)\n",
      "#Bins = 9, Accuracy = 0.937 (+/- 0.006)\n",
      "LR\n",
      "#Bins = 2, Accuracy = 0.892 (+/- 0.008)\n",
      "#Bins = 3, Accuracy = 0.914 (+/- 0.008)\n",
      "#Bins = 4, Accuracy = 0.908 (+/- 0.005)\n",
      "#Bins = 5, Accuracy = 0.917 (+/- 0.009)\n",
      "#Bins = 6, Accuracy = 0.925 (+/- 0.007)\n",
      "#Bins = 7, Accuracy = 0.940 (+/- 0.002)\n",
      "#Bins = 8, Accuracy = 0.925 (+/- 0.007)\n",
      "#Bins = 9, Accuracy = 0.930 (+/- 0.005)\n",
      "NB\n",
      "#Bins = 2, Accuracy = 0.853 (+/- 0.014)\n",
      "#Bins = 3, Accuracy = 0.882 (+/- 0.013)\n",
      "#Bins = 4, Accuracy = 0.870 (+/- 0.012)\n",
      "#Bins = 5, Accuracy = 0.884 (+/- 0.010)\n",
      "#Bins = 6, Accuracy = 0.890 (+/- 0.008)\n",
      "#Bins = 7, Accuracy = 0.904 (+/- 0.006)\n",
      "#Bins = 8, Accuracy = 0.898 (+/- 0.006)\n",
      "#Bins = 9, Accuracy = 0.902 (+/- 0.008)\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(efb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "#Bins = 5, Accuracy = 0.933 (+/- 0.018)\n",
      "#Bins = 10, Accuracy = 0.938 (+/- 0.005)\n",
      "#Bins = 15, Accuracy = 0.943 (+/- 0.016)\n",
      "#Bins = 20, Accuracy = 0.943 (+/- 0.013)\n",
      "LR\n",
      "#Bins = 5, Accuracy = 0.910 (+/- 0.017)\n",
      "#Bins = 10, Accuracy = 0.922 (+/- 0.001)\n",
      "#Bins = 15, Accuracy = 0.949 (+/- 0.007)\n",
      "#Bins = 20, Accuracy = 0.941 (+/- 0.023)\n",
      "NB\n",
      "#Bins = 5, Accuracy = 0.871 (+/- 0.024)\n",
      "#Bins = 10, Accuracy = 0.884 (+/- 0.015)\n",
      "#Bins = 15, Accuracy = 0.916 (+/- 0.011)\n",
      "#Bins = 20, Accuracy = 0.895 (+/- 0.050)\n"
     ]
    }
   ],
   "source": [
    "compute_cv_score_by_n_bins(sb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Iteration = 10000, Loss = 0.3017544746398926\n",
      ">>> Iteration = 20000, Loss = 0.30021047592163086\n",
      ">>> Iteration = 30000, Loss = 0.3046160042285919\n",
      ">>> Iteration = 40000, Loss = 0.30355164408683777\n",
      ">>> Iteration = 50000, Loss = 0.3017103970050812\n",
      ">>> Iteration = 60000, Loss = 0.307841032743454\n",
      ">>> Iteration = 70000, Loss = 0.30641624331474304\n",
      ">>> Iteration = 80000, Loss = 0.30312803387641907\n",
      ">>> Iteration = 90000, Loss = 0.3049249053001404\n",
      ">>> Iteration = 100000, Loss = 0.3034362494945526\n",
      ">>> Iteration = 110000, Loss = 0.3036462068557739\n",
      ">>> Iteration = 120000, Loss = 0.3026193380355835\n",
      ">>> Iteration = 130000, Loss = 0.2992166578769684\n",
      ">>> Iteration = 140000, Loss = 0.3016679883003235\n",
      ">>> Iteration = 150000, Loss = 0.30351972579956055\n",
      ">>> Iteration = 160000, Loss = 0.30218303203582764\n",
      ">>> Iteration = 170000, Loss = 0.3054898977279663\n",
      ">>> Iteration = 180000, Loss = 0.29904675483703613\n",
      ">>> Iteration = 190000, Loss = 0.3019198775291443\n",
      ">>> Iteration = 200000, Loss = 0.3020668923854828\n",
      ">>> Iteration = 210000, Loss = 0.30384257435798645\n",
      ">>> Iteration = 220000, Loss = 0.30325886607170105\n",
      ">>> Iteration = 230000, Loss = 0.30320534110069275\n",
      ">>> Iteration = 240000, Loss = 0.3045049011707306\n",
      ">>> Iteration = 250000, Loss = 0.3026994466781616\n",
      ">>> Iteration = 260000, Loss = 0.30093711614608765\n",
      ">>> Iteration = 270000, Loss = 0.30084821581840515\n",
      ">>> Iteration = 280000, Loss = 0.3008190393447876\n",
      ">>> Iteration = 290000, Loss = 0.30094918608665466\n",
      ">>> Iteration = 300000, Loss = 0.2996760308742523\n",
      "Learning Embedding Finished!\n",
      "Clustering Performance By Discretization Method\n",
      "> Clustering Algorithm = kmeans\n",
      ">> Dummy Coding Only = 0.001669739101139292\n",
      ">> Scale Numerical Vars = 0.0003765429980202118\n",
      ">> Equal Width Binning = 0.013115940557326494\n",
      ">> Equal Freq Binning = 0.013115940557326494\n",
      ">> Semantic Binning = 0.11351939752789995\n",
      "> Clustering Algorithm = agglomerative\n",
      ">> Dummy Coding Only = 0.010268094012067937\n",
      ">> Scale Numerical Vars = 0.2091474260339319\n",
      ">> Equal Width Binning = 0.23036512774761073\n",
      ">> Equal Freq Binning = 0.22574974749163562\n",
      ">> Semantic Binning = 0.2602553393283718\n",
      "CPU times: user 15min 30s, sys: 30.1 s, total: 16min\n",
      "Wall time: 15min 42s\n"
     ]
    }
   ],
   "source": [
    "%time exp.clustering_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.896 (+/- 0.004)\n",
      ">> equal_width\n",
      "Accuracy: 0.942 (+/- 0.006)\n",
      ">> equal_freq\n",
      "Accuracy: 0.937 (+/- 0.002)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.908 (+/- 0.010)\n",
      "CPU times: user 41min 52s, sys: 50.2 s, total: 42min 42s\n",
      "Wall time: 42min 40s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(LogisticRegression(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.975 (+/- 0.001)\n",
      ">> equal_width\n",
      "Accuracy: 0.967 (+/- 0.004)\n",
      ">> equal_freq\n",
      "Accuracy: 0.964 (+/- 0.005)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.964 (+/- 0.001)\n",
      "CPU times: user 43min 50s, sys: 43.9 s, total: 44min 34s\n",
      "Wall time: 44min 44s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(DecisionTreeClassifier(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 fold cv score\n",
      ">> dummy_only\n",
      "Accuracy: 0.879 (+/- 0.007)\n",
      ">> equal_width\n",
      "Accuracy: 0.920 (+/- 0.006)\n",
      ">> equal_freq\n",
      "Accuracy: 0.907 (+/- 0.005)\n",
      ">> semantic_binning\n",
      "Accuracy: 0.855 (+/- 0.010)\n",
      "CPU times: user 45min 21s, sys: 44.8 s, total: 46min 6s\n",
      "Wall time: 46min 20s\n"
     ]
    }
   ],
   "source": [
    "%time get_cv_error_for_model(BernoulliNB(), hr_data, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
