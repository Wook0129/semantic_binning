{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "        \n",
    "    def __init__(self, data, var_dict):\n",
    "        \n",
    "        self.categorical_vars = data[var_dict['categorical_vars']].astype(str)\n",
    "        self.integer_vars = data[var_dict['integer_vars']].astype(np.int32)\n",
    "        self.continuous_vars = data[var_dict['continuous_vars']].astype(np.float32)\n",
    "        self.class_var = data[var_dict['class_var']]\n",
    "        self.input_vars = var_dict['categorical_vars'] + var_dict['continuous_vars'] + var_dict['integer_vars']\n",
    "        self.n_variables = len(self.input_vars)\n",
    "        \n",
    "    def get_dummy_coded_data(self, init_discretize_method='equal_freq', \n",
    "                             n_init_bins=20, bins_by_variable=None):\n",
    "        \n",
    "        continuous_vars = self.continuous_vars.copy()\n",
    "        integer_vars = self.integer_vars.copy()\n",
    "        categorical_vars = pd.get_dummies(self.categorical_vars.copy())\n",
    "        \n",
    "        if not bins_by_variable:\n",
    "            \n",
    "            if init_discretize_method == 'equal_width':\n",
    "                for var in self.continuous_vars.columns:\n",
    "                    continuous_vars[var] = pd.cut(continuous_vars[var], bins=n_init_bins)\n",
    "            elif init_discretize_method == 'equal_freq':\n",
    "                for var in self.continuous_vars.columns:\n",
    "                    continuous_vars[var] = pd.qcut(continuous_vars[var], q=n_init_bins)\n",
    "            elif init_discretize_method == 'scale_numeric':\n",
    "                mean, std = continuous_vars.mean(), continuous_vars.std()\n",
    "                continuous_vars = (continuous_vars - mean) / std\n",
    "            elif init_discretize_method == 'dummy_only':\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        else:\n",
    "            for var in bins_by_variable:\n",
    "                \n",
    "                if var in self.continuous_vars.columns:\n",
    "                    bins = bins_by_variable[var]['split_point']\n",
    "                    continuous_vars[var] = pd.cut(continuous_vars[var], bins=bins)\n",
    "                \n",
    "                elif var in self.integer_vars.columns:\n",
    "                    \n",
    "                    for merged_bin in bins_by_variable[var]['bins']:\n",
    "                        bin_name = [' <OR> '.join([str(x) for x in merged_bin])]\n",
    "                        is_in_bin = integer_vars[var].apply(lambda x: x in merged_bin)\n",
    "                        integer_vars[bin_name] = is_in_bin\n",
    "                    integer_vars.drop(var, axis=1, inplace=True)\n",
    "                    \n",
    "                else:\n",
    "                    for merged_bin in bins_by_variable[var]['bins']:\n",
    "                        cols = ['{}_{}'.format(var, x) for x in merged_bin.split(' <OR> ')]\n",
    "                        if len(cols) >= 2:\n",
    "                            categorical_vars[merged_bin] = categorical_vars[cols].sum(axis=1)\n",
    "                            categorical_vars.drop(cols, axis=1, inplace=True)\n",
    "                    \n",
    "        continuous_vars = pd.get_dummies(continuous_vars)\n",
    "    \n",
    "        return pd.concat([categorical_vars, integer_vars, continuous_vars], axis=1)\n",
    "    \n",
    "    def get_bins_by_variable_from_data(self, dummy_coded_data):\n",
    "\n",
    "        def get_variable_name(dummy_variable_name):\n",
    "            return '_'.join(dummy_variable_name.split('_')[:-1])\n",
    "\n",
    "        def get_interval_and_split_points(dummy_variable_name):\n",
    "            interval = dummy_variable_name.split('_')[-1]\n",
    "            begin = float(interval.split(', ')[0].replace('(',''))\n",
    "            end = float(interval.split(', ')[1].replace(']',''))\n",
    "            return interval, begin, end\n",
    "    \n",
    "        bins_by_variable = dict()\n",
    "\n",
    "        for var in self.continuous_vars.columns:\n",
    "\n",
    "            bins = []\n",
    "            split_points = set()\n",
    "\n",
    "            dummy_vars = [x for x in dummy_coded_data.columns \n",
    "                          if var == get_variable_name(x)]\n",
    "\n",
    "            for dummy_var in dummy_vars:\n",
    "                interval, begin, end = get_interval_and_split_points(dummy_var)\n",
    "                bins.append(interval)\n",
    "                split_points.update([begin, end])\n",
    "\n",
    "            split_points = sorted(split_points)\n",
    "\n",
    "            bins_by_variable[var] = dict(bins=bins, split_point=split_points)\n",
    "\n",
    "        for var in self.integer_vars.colums:\n",
    "            \n",
    "            bins = []\n",
    "            dummy_vars = [x for x in dummy_coded_data.columns\n",
    "                         if var == get_variable_name(x)]\n",
    "            \n",
    "            for dummy_var in dummy_vars:\n",
    "                values_in_bin = [int(x) for x in dummy_var[len(var)+1:].split(' <OR> ')]\n",
    "\n",
    "                \n",
    "        return bins_by_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from embed_bins import BinEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = dict(\n",
    "    categorical_vars = ['Work_accident', 'promotion_last_5years', 'sales', 'salary'],\n",
    "    integer_vars = ['number_project','time_spend_company'],\n",
    "    continuous_vars = ['satisfaction_level', 'last_evaluation', 'average_montly_hours'],\n",
    "    class_var = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(pd.read_csv('data/HR_comma_sep.csv'), var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_coded = data_handler.get_dummy_coded_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Work_accident_0', 'Work_accident_1', 'promotion_last_5years_0',\n",
       "       'promotion_last_5years_1', 'sales_IT', 'sales_RandD',\n",
       "       'sales_accounting', 'sales_hr', 'sales_management', 'sales_marketing',\n",
       "       'sales_product_mng', 'sales_sales', 'sales_support', 'sales_technical',\n",
       "       'salary_high', 'salary_low', 'salary_medium', 'number_project',\n",
       "       'time_spend_company', 'satisfaction_level_(0.089, 0.11]',\n",
       "       'satisfaction_level_(0.11, 0.21]', 'satisfaction_level_(0.21, 0.36]',\n",
       "       'satisfaction_level_(0.36, 0.4]', 'satisfaction_level_(0.4, 0.44]',\n",
       "       'satisfaction_level_(0.44, 0.49]', 'satisfaction_level_(0.49, 0.53]',\n",
       "       'satisfaction_level_(0.53, 0.57]', 'satisfaction_level_(0.57, 0.61]',\n",
       "       'satisfaction_level_(0.61, 0.64]', 'satisfaction_level_(0.64, 0.68]',\n",
       "       'satisfaction_level_(0.68, 0.72]', 'satisfaction_level_(0.72, 0.75]',\n",
       "       'satisfaction_level_(0.75, 0.78]', 'satisfaction_level_(0.78, 0.82]',\n",
       "       'satisfaction_level_(0.82, 0.85]', 'satisfaction_level_(0.85, 0.88]',\n",
       "       'satisfaction_level_(0.88, 0.92]', 'satisfaction_level_(0.92, 0.96]',\n",
       "       'satisfaction_level_(0.96, 1.0]', 'last_evaluation_(0.359, 0.46]',\n",
       "       'last_evaluation_(0.46, 0.49]', 'last_evaluation_(0.49, 0.51]',\n",
       "       'last_evaluation_(0.51, 0.54]', 'last_evaluation_(0.54, 0.56]',\n",
       "       'last_evaluation_(0.56, 0.59]', 'last_evaluation_(0.59, 0.62]',\n",
       "       'last_evaluation_(0.62, 0.65]', 'last_evaluation_(0.65, 0.68]',\n",
       "       'last_evaluation_(0.68, 0.72]', 'last_evaluation_(0.72, 0.75]',\n",
       "       'last_evaluation_(0.75, 0.78]', 'last_evaluation_(0.78, 0.81]',\n",
       "       'last_evaluation_(0.81, 0.84]', 'last_evaluation_(0.84, 0.87]',\n",
       "       'last_evaluation_(0.87, 0.89]', 'last_evaluation_(0.89, 0.92]',\n",
       "       'last_evaluation_(0.92, 0.95]', 'last_evaluation_(0.95, 0.98]',\n",
       "       'last_evaluation_(0.98, 1.0]', 'average_montly_hours_(95.999, 130.0]',\n",
       "       'average_montly_hours_(130.0, 137.0]',\n",
       "       'average_montly_hours_(137.0, 144.0]',\n",
       "       'average_montly_hours_(144.0, 150.0]',\n",
       "       'average_montly_hours_(150.0, 156.0]',\n",
       "       'average_montly_hours_(156.0, 162.0]',\n",
       "       'average_montly_hours_(162.0, 171.0]',\n",
       "       'average_montly_hours_(171.0, 181.0]',\n",
       "       'average_montly_hours_(181.0, 190.0]',\n",
       "       'average_montly_hours_(190.0, 200.0]',\n",
       "       'average_montly_hours_(200.0, 210.0]',\n",
       "       'average_montly_hours_(210.0, 220.0]',\n",
       "       'average_montly_hours_(220.0, 229.0]',\n",
       "       'average_montly_hours_(229.0, 238.0]',\n",
       "       'average_montly_hours_(238.0, 245.0]',\n",
       "       'average_montly_hours_(245.0, 253.0]',\n",
       "       'average_montly_hours_(253.0, 260.0]',\n",
       "       'average_montly_hours_(260.0, 267.0]',\n",
       "       'average_montly_hours_(267.0, 275.0]',\n",
       "       'average_montly_hours_(275.0, 310.0]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_coded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_embedder = BinEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 716568 is out of bounds for axis 1 with size 629958",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-b57118988ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbin_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_bin_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_coded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/taewook/workspace/semantic_binning/embed_bins.py\u001b[0m in \u001b[0;36mlearn_bin_embeddings\u001b[0;34m(self, dummy_coded_data, n_variables, embedding_dim, lr, n_epoch, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/taewook/workspace/semantic_binning/batch_generator.py\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 716568 is out of bounds for axis 1 with size 629958"
     ]
    }
   ],
   "source": [
    "bin_embedder.learn_bin_embeddings(dummy_coded, data_handler.n_variables, n_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "class BinMerger:\n",
    "    \n",
    "    def __init__(self, embedding_by_column, clustering_method='agglomerative'):\n",
    "        self.embedding_by_column = embedding_by_column\n",
    "        if clustering_method in ['kmeans', 'agglomerative']:\n",
    "            self.clustering_method = clustering_method\n",
    "        else:\n",
    "            raise ValueError('Available method = [kmeans, agglomerative]')        \n",
    "    \n",
    "    def _get_cols_and_embeddings(self, variable):\n",
    "        cols = []\n",
    "        embeddings = []\n",
    "        for c, e in self.embedding_by_column.items():\n",
    "            if variable in c:\n",
    "                cols.append(c)\n",
    "                embeddings.append(e)\n",
    "        return cols, embeddings\n",
    "\n",
    "    def _cluster_embeddings(self, embeddings):\n",
    "        \n",
    "        # Determine Optimal Number of Cluster\n",
    "        scores = []\n",
    "        \n",
    "        for n_cluster in range(2, len(embeddings)):\n",
    "            \n",
    "            if self.clustering_method == 'kmeans':\n",
    "                cluster_label = KMeans(n_cluster).fit_predict(embeddings)\n",
    "            if self.clustering_method == 'agglomerative':\n",
    "                cluster_label = AgglomerativeClustering(n_cluster).fit_predict(embeddings)\n",
    "                \n",
    "            score = silhouette_score(embeddings, cluster_label)\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Clustering with Optimal Number of Cluster\n",
    "        best_n = np.argmax(scores) + 2\n",
    "        if self.clustering_method == 'kmeans':\n",
    "            cluster_label = KMeans(best_n).fit_predict(embeddings)\n",
    "        if self.clustering_method == 'agglomerative':\n",
    "            cluster_label = AgglomerativeClustering(best_n).fit_predict(embeddings)\n",
    "        \n",
    "        return cluster_label\n",
    "\n",
    "    def _get_cols_by_cluster(self, cols, cluster_label, v_type):\n",
    "        \n",
    "        cols_by_cluster = dict()\n",
    "\n",
    "        if v_type == 'continuous':\n",
    "            cnt, prev_label = -1, -1\n",
    "            for col, label in sorted(zip(cols, cluster_label), key=lambda x:x[0]):\n",
    "                if prev_label == label:\n",
    "                    cols_by_cluster[cnt].append(col)\n",
    "                else:\n",
    "                    cnt += 1\n",
    "                    cols_by_cluster[cnt] = [col]\n",
    "                prev_label = label            \n",
    "        \n",
    "        elif v_type == 'integer':\n",
    "            for col, label in zip(cols, cluster_label):\n",
    "                if label in cols_by_cluster:\n",
    "                    cols_by_cluster[label].append(col)\n",
    "                else:\n",
    "                    cols_by_cluster[label] = [col]\n",
    "            \n",
    "        elif v_type == 'categorical':\n",
    "            for col, label in zip(cols, cluster_label):\n",
    "                if label in cols_by_cluster:\n",
    "                    cols_by_cluster[label].append(col)\n",
    "                else:\n",
    "                    cols_by_cluster[label] = [col]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Available v_type = [continuous, integer, categorical]')\n",
    "\n",
    "        return cols_by_cluster\n",
    "\n",
    "    def _merge_bins(self, variable, v_type):\n",
    "        \n",
    "        def get_category_level_name(variable, col_name):\n",
    "            return col_name[len(variable) + 1:]\n",
    "        \n",
    "        merged_bins = list()\n",
    "        split_points = set()\n",
    "        \n",
    "        cols, embeddings = self._get_cols_and_embeddings(variable)\n",
    "        \n",
    "        # Do not Merge, if #Bins <= 2\n",
    "        if v_type == 'categorical' and (len(cols) <= 2):\n",
    "            merged_bins = [get_category_level_name(variable, x) for x in cols]\n",
    "            return merged_bins, split_points\n",
    "        \n",
    "        cluster_label = self._cluster_embeddings(embeddings)\n",
    "        cols_by_cluster = self._get_cols_by_cluster(cols, cluster_label, v_type)\n",
    "        \n",
    "        for cols in cols_by_cluster.values():\n",
    "        \n",
    "            if v_type == 'continuous':\n",
    "                intervals = [get_category_level_name(variable, x) for x in cols]\n",
    "                begin = intervals[0].split(' ')[0]\n",
    "                end = intervals[-1].split(' ')[1]\n",
    "                merged_bins.append(' '.join([begin, end]))\n",
    "\n",
    "                begin_point = float(begin.replace('(','').replace(',',''))\n",
    "                end_point = float(end.replace(']','').replace(',',''))\n",
    "                split_points.update([begin_point, end_point])\n",
    "            \n",
    "            if v_type == 'integer':\n",
    "                values_in_bin = sorted([int(get_category_level_name(variable, x)) for x in cols])\n",
    "                \n",
    "                current_bin = []\n",
    "                prev_value = None\n",
    "\n",
    "                for value in values_in_bin:\n",
    "                    if (value - 1) == prev_value:\n",
    "                        current_bin.append(value)\n",
    "                    else:\n",
    "                        if len(current_bin) > 0:\n",
    "                            merged_bins.append(current_bin)\n",
    "                        current_bin = [value]\n",
    "                    prev_value = value\n",
    "\n",
    "                merged_bins.append(current_bin)\n",
    "                \n",
    "            if v_type == 'categorical':\n",
    "                category_levels = [get_category_level_name(variable, x) for x in cols]\n",
    "                merged_bins.append(' <OR> '.join(category_levels))\n",
    "                \n",
    "        split_points = sorted(split_points)\n",
    "        \n",
    "        return merged_bins, split_points\n",
    "    \n",
    "    def get_merged_bins_by_var(self, var_dict, \n",
    "                               merge_continuous_var=True, \n",
    "                               merge_integer_vars=True,\n",
    "                               merge_categorical_var=True):\n",
    "        \n",
    "        bins_by_variable = dict()\n",
    "        \n",
    "        if merge_continuous_var:\n",
    "            for var in var_dict['continuous_vars']:                \n",
    "                merged_bins, split_points = self._merge_bins(var, v_type='continuous')\n",
    "                bins_by_variable[var] = dict(bins=merged_bins, split_point=split_points)\n",
    "        \n",
    "        if merge_integer_vars:\n",
    "            for var in var_dict['integer_vars']:                    \n",
    "                merged_bins, _ = self._merge_bins(var, v_type='integer')\n",
    "                bins_by_variable[var] = dict(bins=merged_bins)\n",
    "            \n",
    "        if merge_categorical_var:\n",
    "            for var in var_dict['categorical_vars']:\n",
    "                merged_bins, _ = self._merge_bins(var, v_type='categorical')\n",
    "                bins_by_variable[var] = dict(bins=merged_bins)\n",
    "            \n",
    "        return bins_by_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_merger = BinMerger(bin_embedder.embedding_by_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_by_var = bin_merger.get_merged_bins_by_var(var_dict, merge_categorical_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_by_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.get_dummy_coded_data(bins_by_variable=bins_by_var).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
